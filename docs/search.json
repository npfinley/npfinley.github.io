[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "Anomaly and Outlier Detection with Clustering Techniques\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClusters with a Gaussian Matrix\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Clustering/Clustering.html",
    "href": "posts/Clustering/Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "hello I am testing how dumb I am\n\n\nCode\nimport sklearn\n\n\nIf I type some more down here how does that go"
  },
  {
    "objectID": "posts/Clustering/Clustering.html#if-i-type-some-more-down-here-how-does-that-go",
    "href": "posts/Clustering/Clustering.html#if-i-type-some-more-down-here-how-does-that-go",
    "title": "Clustering",
    "section": "IF I type some more down here how does that go",
    "text": "IF I type some more down here how does that go"
  },
  {
    "objectID": "GaussianMatrix.html",
    "href": "GaussianMatrix.html",
    "title": "Patrick Finley's Blog",
    "section": "",
    "text": "import sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n\nfrom pathlib import Path\n\nIMAGES_PATH = Path() / \"images\" / \"ForGaussianMatrix\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n\nimport os\nos.environ[\"MKL_NUM_THREADS\"] = \"1\" \nos.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" \nos.environ[\"OMP_NUM_THREADS\"] = \"6\" \n\n\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\nplt.grid()\nsave_fig(\"blobs_plot\")\nplt.show()\n\n\n\n\n\nfrom sklearn.mixture import GaussianMixture\n\n\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\nGaussianMixture(n_components=4, n_init=10, random_state=27)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=4, n_init=10, random_state=27)\n\n\n\ngm.weights_\n\narray([0.19838075, 0.20086554, 0.30224528, 0.29850842])\n\n\n\ngm.means_\n\narray([[-2.46484766,  1.28737693],\n       [ 3.68999774,  2.36417118],\n       [-1.20619237, -1.17594382],\n       [ 1.21207915,  3.59365466]])\n\n\n\ngm.covariances_\n\narray([[[1.08547535, 0.07669351],\n        [0.07669351, 0.93922256]],\n\n       [[1.07142397, 0.02135635],\n        [0.02135635, 1.15576698]],\n\n       [[0.17377385, 0.14322807],\n        [0.14322807, 0.18633513]],\n\n       [[0.2210167 , 0.17509654],\n        [0.17509654, 0.21530691]]])\n\n\n\ngm.converged_\n\nTrue\n\n\n\ngm.n_iter_\n\n4\n\n\n\ngm.predict(X)\n\narray([3, 3, 3, ..., 1, 1, 1], dtype=int64)\n\n\n\ngm.predict_proba(X).round(3)\n\narray([[0.   , 0.003, 0.   , 0.997],\n       [0.   , 0.466, 0.   , 0.534],\n       [0.001, 0.019, 0.   , 0.979],\n       ...,\n       [0.   , 1.   , 0.   , 0.   ],\n       [0.   , 1.   , 0.   , 0.   ],\n       [0.   , 1.   , 0.   , 0.   ]])\n\n\n\nX_new, y_new = gm.sample(6)\nX_new\n\narray([[-2.52953419,  2.26392006],\n       [ 3.04258093,  3.26937155],\n       [ 4.20922236,  3.09372466],\n       [-1.45605325, -1.26906405],\n       [ 1.27432055,  3.73269449],\n       [ 1.91022481,  3.84924244]])\n\n\n\ny_new\n\narray([0, 1, 1, 2, 3, 3])\n\n\n\ngm.score_samples(X).round(2)\n\narray([-1.28, -4.82, -3.2 , ..., -3.84, -3.82, -3.77])\n\n\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z,\n                 norm=LogNorm(vmin=1.0, vmax=30.0),\n                 levels=np.logspace(0, 2, 12))\n    plt.contour(xx, yy, Z,\n                norm=LogNorm(vmin=1.0, vmax=30.0),\n                levels=np.logspace(0, 2, 12),\n                linewidths=1, colors='k')\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nsave_fig(\"gaussian_mixtures_plot\")\nplt.show()\n\n\n\n\n\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"bo-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"go--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.legend()\nplt.grid()\nsave_fig(\"aic_bic_vs_k_plot\")\nplt.show()\n\n\n\n\n\n\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 2)\nanomalies = X[densities &lt; density_threshold]\n\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')\nplt.ylim(top=5.1)\n\nsave_fig(\"mixture_anomaly_detection_plot\")\nplt.show()"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clusters with a Gaussian Matrix",
    "section": "",
    "text": "import sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\nClustering is a technique used to help us find trends in complex data sets that might be hard to identify manually. With the help of machine learning techniques such as DBScan, KMeans, Gausian Matrix and others we can group the data into clusters that we can then work to interpret. Classifcation is another technique that also breaks up data sets into distinct groups based on different criteria, but with classification there tends to be groups we are setting out to fit the data into. Clustering on the other hand works with data that we have little to no information about before we start working with. With clustering the goal is to look at the clusters and figure out what seperates on cluster from another.\nFor example clustering might be used to help inform an e-commerce site on what they may want to add or adjust based on the tendencies of current customers. They could take the purchase history or viewing history of users and generate clusters. From there, they can analyze them to see if there is specific interest in a certain types of products. From there they could better design their website to meet the needs of their customers.\n\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\n\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\nplt.grid()\nplt.show()\n\n\n\n\nTo help demonstrate what the process of using a clustering technique might look like I have generated some data using sklearn’s make_blobs function. This data is completely meaningless, but that with clustering that is completely fine. By looking at the plot of the data we can pretty easily see that there are 4 clusters to consider. Two of them are rather closely packed and two are rather spread out.\nWhen considering what clustering technque there are three that came to mind: KMeans, dbscan, and Gaussian Matrix. KMeans would struggle with this particular data set because two of the clusters are not normal or circular. Dbscan would also struggle due to the rather spread out nature of the other two clusters. Gaussian matrix though should be able to deal with this data set well as it is not anticipating the clusters being circular and not based on the distance seperating one specific data point from another. As a result, I will be using a gaussian matrix to help assign clusters and demonstrate this technique.\nThere are two main criteria that we want to give to the program in order to set up the gaussian matrix: number of mixture components and the number of initilizations to perform. There are other parameters we can set such as the convariance type, but for this particular data set the defaults for this should be good.\nThe number of mixture components should match out expected number of clusters. Given that there are pretty clearly four distinct clusters we could just use four for the number of mixture components. However, we can also plot either the Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) across multiple different values to figure out what this value should be. ********* MORE HERE FOR THE CALCS ************\n\nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"o-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"o--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nIn this case, I have used sklearn in order to calculate these statistics, but the actual formulas for this are as follows:\nBIC = k ln(n) - 2 ln(L) AIC = 2 k - 2 ln(L)\nL is the maximum value of the likelihood function for the model k is the number of estimated parameters in the model n is the sample size\nBy looking at the minimum point on either of these lines, we can see that a value of four for the number of mixture components would be best. This helps us to reduce the chance of potentially overfitting the model which would make it harder for us to interpret results accurately.\nFor now, we will just use ten for the number of initilizations, but this can be varied to see how the results turn out best later. With this input we can go ahead and fit a matrix to the data and plot the results using the sklearn and matplotlib libraries.\n\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\nGaussianMixture(n_components=4, n_init=10, random_state=27)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=4, n_init=10, random_state=27)\n\n\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nplt.show()\n\n\n\n\nOnce we have fit the data we can go ahead and plot it again along with the centroids and decision boundries the algorithm came up with. Now, we can see that the algorithm did a very good job of fitting to the four distinct clusters. We can see the red dashed lines around each of the more densely packed clusters and the boundry between the two more sparsely packed clusters. From this, we can start analyzing the clusters to see distinct similariets between the groups.\nFor example, if this data was the search habits of different users on an online pharmacy we might be able to see that distinct age ranges are interested in certain products. This might give us the idea to potentially break up products based on age. I we were using some sort of dimendionality reduction then maybe we could see that other factors may impact interest such as weight or gender, but that will have to wait till a later blog post."
  },
  {
    "objectID": "posts/Clustering/clustering.html",
    "href": "posts/Clustering/clustering.html",
    "title": "Clusters with a Gaussian Matrix",
    "section": "",
    "text": "Code\nimport sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\nClustering is a technique used to help us find trends in complex data sets that might be hard to identify manually. With the help of machine learning techniques such as DBScan, KMeans, Gausian Matrix and others we can group the data into clusters that we can then work to interpret. Classifcation is another technique that also breaks up data sets into distinct groups based on different criteria, but with classification there tends to be groups we are setting out to fit the data into. Clustering on the other hand works with data that we have little to no information about before we start working with. With clustering the goal is to look at the clusters and figure out what seperates on cluster from another.\nFor example clustering might be used to help inform an e-commerce site on what they may want to add or adjust based on the tendencies of current customers. They could take the purchase history or viewing history of users and generate clusters. From there, they can analyze them to see if there is specific interest in a certain types of products. From there they could better design their website to meet the needs of their customers.\n\n\nCode\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\n\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\nplt.grid()\nplt.show()\n\n\n\n\n\nTo help demonstrate what the process of using a clustering technique might look like I have generated some data using sklearn’s make_blobs function. This data is completely meaningless, but that with clustering that is completely fine. By looking at the plot of the data we can pretty easily see that there are 4 clusters to consider. Two of them are rather closely packed and two are rather spread out.\nWhen considering what clustering technque there are three that came to mind: KMeans, dbscan, and Gaussian Matrix. KMeans would struggle with this particular data set because two of the clusters are not normal or circular. Dbscan would also struggle due to the rather spread out nature of the other two clusters. Gaussian matrix though should be able to deal with this data set well as it is not anticipating the clusters being circular and not based on the distance seperating one specific data point from another. As a result, I will be using a gaussian matrix to help assign clusters and demonstrate this technique.\nThere are two main criteria that we want to give to the program in order to set up the gaussian matrix: number of mixture components and the number of initilizations to perform. There are other parameters we can set such as the convariance type, but for this particular data set the defaults for this should be good.\nThe number of mixture components should match out expected number of clusters. Given that there are pretty clearly four distinct clusters we could just use four for the number of mixture components. However, we can also plot either the Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) across multiple different values to figure out what this value should be.\n\n\nCode\nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"o-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"o--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\nIn this case, I have used sklearn in order to calculate these statistics, but the actual formulas for this are as follows:\nBIC = k ln(n) - 2 ln(L) AIC = 2 k - 2 ln(L)\nL is the maximum value of the likelihood function for the model k is the number of estimated parameters in the model n is the sample size\nBy looking at the minimum point on either of these lines, we can see that a value of four for the number of mixture components would be best. This helps us to reduce the chance of potentially overfitting the model which would make it harder for us to interpret results accurately.\nFor now, we will just use ten for the number of initilizations, but this can be varied to see how the results turn out best later. With this input we can go ahead and fit a matrix to the data and plot the results using the sklearn and matplotlib libraries.\n\n\nCode\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\n\nGaussianMixture(n_components=4, n_init=10, random_state=27)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=4, n_init=10, random_state=27)\n\n\n\n\nCode\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nplt.show()\n\n\n\n\n\nOnce we have fit the data we can go ahead and plot it again along with the centroids and decision boundries the algorithm came up with. Now, we can see that the algorithm did a very good job of fitting to the four distinct clusters. We can see the red dashed lines around each of the more densely packed clusters and the boundry between the two more sparsely packed clusters. From this, we can start analyzing the clusters to see distinct similariets between the groups.\nFor example, if this data was the search habits of different users on an online pharmacy we might be able to see that distinct age ranges are interested in certain products. This might give us the idea to potentially break up products based on age. I we were using some sort of dimendionality reduction then maybe we could see that other factors may impact interest such as weight or gender, but that will have to wait till a later blog post."
  },
  {
    "objectID": "posts/anomolyDetection/anomolyDetection.html",
    "href": "posts/anomolyDetection/anomolyDetection.html",
    "title": "Anomaly and Outlier Detection with Clustering Techniques",
    "section": "",
    "text": "Clutering and other machine learning techniques are great for helping us to determine trends in data that we can then use to better inform our decisions. This is an extremely useful tool for just about any field that exists as it can greatly speed up processes for workers. However, with any data set there will generally be outliers that cannot be easily explained. This can be due to any number of different things, but we need to be able to identify these as they can make the decision making process more challenging.\nDoing this can be tough as mistakes in anomoly detection can lead to valid data potentially being overlooked in our findings. Depedning on the technique we are using the way to figure out which points are outliers. For example, with dbscan the outliers are determined as part of the process of fitting the data to the model. This is due to the fact that dbscan works by considering the distance between a point and its nearest neighbors. If the next nearest point is within the preset limits then it adds it to the same cluster and determines it’s neighbors until it finds all of the points in that cluster. Once the the algorthim is finished, any points that were to far from the rest are not included in any of the clusters and are then considered anomolies or outliers. With tools like sklearn we can actually get a data set of just these points that we can then work with to determine if they are infact outliers or if the model needs to be adjusted to better include the points. Of course, for some applications this is not acceptable as you cannot garuntee that every point will be included in the analysis and still get accurate results.\nWith other types of methods, the process of determining outliers is a bit more complex. For example, both KMeans and gaussian matrix techniques will include all data points in at least one cluster that they find. This makes them ideal for situations in which we need to be sure that every data point is considered, but we have to come up with other ways to pick out outliers. To demonstrate this, I am going to use the data and model from a previous post about clustering.\n\n\nCode\nimport sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n\nIMAGES_PATH = Path() / \"images\" / \"ForGaussianMatrix\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n    \n    \nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\n\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='+', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\n\nplt.show()\n\n\n\n\n\nThis data has been generated using sklearn’s makeblob function. There are four distinct clusters in this data which I have used a gaussian matrix to seperate from each other. The crosses mark the centroids of each of the distinct clusters. The red dashed lines represent the decision boundaries of each of the clusters in between which each point is considered to be in that cluster.\nIt is pretty clear that some of these points are very distant from their respective cluster or potentially between a few different ones. We will probably want to take note of these or even exclude them from our considerations as we work with the data. The simplest way to do this is to essentially set a threshhold beyond which if any points do not meet the threshold they will be considered an outlier. This threshold can be determined by many things. If we are working in a production environment or somewhere were the data input could be wrong we might expect a specific percentage of the data to be anomolies or outliers. We can use this expected rate as our threshold.\nWith this in mind, we will first calculate the density or log-likelihood of each sample. Then we will calculate what the threshold value is and apply it to the data to figure out which points do not meet it.\n\n\nCode\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 2)\nanomalies = X[densities &lt; density_threshold]\n\n\n\n\nCode\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='x')\nplt.ylim(top=5.1)\n\n\nplt.show()\n\n\n\n\n\nWith the points seperated, we can use matplot to plot markers for each of the outliers as can be seen above marked by the red “x” marks. By marking these we can see that most of the points do in fact sit far from their respective culster centroids which makes them less likely to be in a cluster and more likely to be an outlier. Then we could take this data and try to understand where they are coming from. This can give us information about our processes to reduce the chance for errors in the future or maybe understand a different user we are not currently catering to well."
  },
  {
    "objectID": "anomolyDetection.html",
    "href": "anomolyDetection.html",
    "title": "Patrick Finley's Blog",
    "section": "",
    "text": "#Anomoly/Outlier Detection in Clustering\nClutering and other machine learning techniques are great for helping us to determine trends in data that we can then use to better inform our decisions. This is an extremely useful tool for just about any field that exists as it can greatly speed up processes for workers. However, with any data set there will generally be outliers that cannot be easily explained. This can be due to any number of different things, but we need to be able to identify these as they can make the decision making process more challenging.\nDoing this can be tough as mistakes in anomoly detection can lead to valid data potentially being overlooked in our findings. Depedning on the technique we are using the way to figure out which points are outliers. For example, with dbscan the outliers are determined as part of the process of fitting the data to the model. This is due to the fact that dbscan works by considering the distance between a point and its nearest neighbors. If the next nearest point is within the preset limits then it adds it to the same cluster and determines it’s neighbors until it finds all of the points in that cluster. Once the the algorthim is finished, any points that were to far from the rest are not included in any of the clusters and are then considered anomolies or outliers. With tools like sklearn we can actually get a data set of just these points that we can then work with to determine if they are infact outliers or if the model needs to be adjusted to better include the points. Of course, for some applications this is not acceptable as you cannot garuntee that every point will be included in the analysis and still get accurate results.\nWith other types of methods, the process of determining outliers is a bit more complex. For example, both KMeans and gaussian matrix techniques will include all data points in at least one cluster that they find. This makes them ideal for situations in which we need to be sure that every data point is considered, but we have to come up with other ways to pick out outliers. To demonstrate this, I am going to use the data and model from a previous post about clustering.\n\nimport sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n\nIMAGES_PATH = Path() / \"images\" / \"ForGaussianMatrix\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n    \n    \nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\n\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='+', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\n\nplt.show()\n\n\n\n\nThis data has been generated using sklearn’s makeblob function. There are four distinct clusters in this data which I have used a gaussian matrix to seperate from each other. The crosses mark the centroids of each of the distinct clusters. The red dashed lines represent the decision boundaries of each of the clusters in between which each point is considered to be in that cluster.\nIt is pretty clear that some of these points are very distant from their respective cluster or potentially between a few different ones. We will probably want to take note of these or even exclude them from our considerations as we work with the data. The simplest way to do this is to essentially set a threshhold beyond which if any points do not meet the threshold they will be considered an outlier. This threshold can be determined by many things. If we are working in a production environment or somewhere were the data input could be wrong we might expect a specific percentage of the data to be anomolies or outliers. We can use this expected rate as our threshold.\nWith this in mind, we will first calculate the density or log-likelihood of each sample. Then we will calculate what the threshold value is and apply it to the data to figure out which points do not meet it.\n\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 2)\nanomalies = X[densities &lt; density_threshold]\n\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='x')\nplt.ylim(top=5.1)\n\n\nplt.show()\n\n\n\n\nWith the points seperated, we can use matplot to plot markers for each of the outliers as can be seen above marked by the red “x” marks. By marking these we can see that most of the points do in fact sit far from their respective culster centroids which makes them less likely to be in a cluster and more likely to be an outlier. Then we could take this data and try to understand where they are coming from. This can give us information about our processes to reduce the chance for errors in the future or maybe understand a different user we are not currently catering to well."
  }
]