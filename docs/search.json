[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "Test Anomaly and Outlier Detection with Clustering Techniques\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClassification with Decision Trees\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClusters with a Gaussian Matrix\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLinear and Non-linear Techniques\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Clustering/Clustering.html",
    "href": "posts/Clustering/Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "hello I am testing how dumb I am\n\n\nCode\nimport sklearn\n\n\nIf I type some more down here how does that go"
  },
  {
    "objectID": "posts/Clustering/Clustering.html#if-i-type-some-more-down-here-how-does-that-go",
    "href": "posts/Clustering/Clustering.html#if-i-type-some-more-down-here-how-does-that-go",
    "title": "Clustering",
    "section": "IF I type some more down here how does that go",
    "text": "IF I type some more down here how does that go"
  },
  {
    "objectID": "GaussianMatrix.html",
    "href": "GaussianMatrix.html",
    "title": "Patrick Finley's Blog",
    "section": "",
    "text": "import sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n\nfrom pathlib import Path\n\nIMAGES_PATH = Path() / \"images\" / \"ForGaussianMatrix\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n\nimport os\nos.environ[\"MKL_NUM_THREADS\"] = \"1\" \nos.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" \nos.environ[\"OMP_NUM_THREADS\"] = \"6\" \n\n\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\nplt.grid()\nsave_fig(\"blobs_plot\")\nplt.show()\n\n\n\n\n\nfrom sklearn.mixture import GaussianMixture\n\n\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\nGaussianMixture(n_components=4, n_init=10, random_state=27)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=4, n_init=10, random_state=27)\n\n\n\ngm.weights_\n\narray([0.19838075, 0.20086554, 0.30224528, 0.29850842])\n\n\n\ngm.means_\n\narray([[-2.46484766,  1.28737693],\n       [ 3.68999774,  2.36417118],\n       [-1.20619237, -1.17594382],\n       [ 1.21207915,  3.59365466]])\n\n\n\ngm.covariances_\n\narray([[[1.08547535, 0.07669351],\n        [0.07669351, 0.93922256]],\n\n       [[1.07142397, 0.02135635],\n        [0.02135635, 1.15576698]],\n\n       [[0.17377385, 0.14322807],\n        [0.14322807, 0.18633513]],\n\n       [[0.2210167 , 0.17509654],\n        [0.17509654, 0.21530691]]])\n\n\n\ngm.converged_\n\nTrue\n\n\n\ngm.n_iter_\n\n4\n\n\n\ngm.predict(X)\n\narray([3, 3, 3, ..., 1, 1, 1], dtype=int64)\n\n\n\ngm.predict_proba(X).round(3)\n\narray([[0.   , 0.003, 0.   , 0.997],\n       [0.   , 0.466, 0.   , 0.534],\n       [0.001, 0.019, 0.   , 0.979],\n       ...,\n       [0.   , 1.   , 0.   , 0.   ],\n       [0.   , 1.   , 0.   , 0.   ],\n       [0.   , 1.   , 0.   , 0.   ]])\n\n\n\nX_new, y_new = gm.sample(6)\nX_new\n\narray([[-2.52953419,  2.26392006],\n       [ 3.04258093,  3.26937155],\n       [ 4.20922236,  3.09372466],\n       [-1.45605325, -1.26906405],\n       [ 1.27432055,  3.73269449],\n       [ 1.91022481,  3.84924244]])\n\n\n\ny_new\n\narray([0, 1, 1, 2, 3, 3])\n\n\n\ngm.score_samples(X).round(2)\n\narray([-1.28, -4.82, -3.2 , ..., -3.84, -3.82, -3.77])\n\n\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z,\n                 norm=LogNorm(vmin=1.0, vmax=30.0),\n                 levels=np.logspace(0, 2, 12))\n    plt.contour(xx, yy, Z,\n                norm=LogNorm(vmin=1.0, vmax=30.0),\n                levels=np.logspace(0, 2, 12),\n                linewidths=1, colors='k')\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nsave_fig(\"gaussian_mixtures_plot\")\nplt.show()\n\n\n\n\n\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"bo-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"go--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.legend()\nplt.grid()\nsave_fig(\"aic_bic_vs_k_plot\")\nplt.show()\n\n\n\n\n\n\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 2)\nanomalies = X[densities &lt; density_threshold]\n\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')\nplt.ylim(top=5.1)\n\nsave_fig(\"mixture_anomaly_detection_plot\")\nplt.show()"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clusters with a Gaussian Matrix",
    "section": "",
    "text": "import sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\nClustering is a technique used to help us find trends in complex data sets that might be hard to identify manually. With the help of machine learning techniques such as DBScan, KMeans, Gausian Matrix and others we can group the data into clusters that we can then work to interpret. Classifcation is another technique that also breaks up data sets into distinct groups based on different criteria, but with classification there tends to be groups we are setting out to fit the data into. Clustering on the other hand works with data that we have little to no information about before we start working with. With clustering the goal is to look at the clusters and figure out what seperates on cluster from another.\nFor example clustering might be used to help inform an e-commerce site on what they may want to add or adjust based on the tendencies of current customers. They could take the purchase history or viewing history of users and generate clusters. From there, they can analyze them to see if there is specific interest in a certain types of products. From there they could better design their website to meet the needs of their customers.\n\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\n\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\nplt.grid()\nplt.show()\n\n\n\n\nTo help demonstrate what the process of using a clustering technique might look like I have generated some data using sklearn’s make_blobs function. This data is completely meaningless, but that with clustering that is completely fine. By looking at the plot of the data we can pretty easily see that there are 4 clusters to consider. Two of them are rather closely packed and two are rather spread out.\nWhen considering what clustering technque there are three that came to mind: KMeans, dbscan, and Gaussian Matrix. KMeans would struggle with this particular data set because two of the clusters are not normal or circular. Dbscan would also struggle due to the rather spread out nature of the other two clusters. Gaussian matrix though should be able to deal with this data set well as it is not anticipating the clusters being circular and not based on the distance seperating one specific data point from another. As a result, I will be using a gaussian matrix to help assign clusters and demonstrate this technique.\nThere are two main criteria that we want to give to the program in order to set up the gaussian matrix: number of mixture components and the number of initilizations to perform. There are other parameters we can set such as the convariance type, but for this particular data set the defaults for this should be good.\nThe number of mixture components should match out expected number of clusters. Given that there are pretty clearly four distinct clusters we could just use four for the number of mixture components. However, we can also plot either the Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) across multiple different values to figure out what this value should be. ********* MORE HERE FOR THE CALCS ************\n\nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"o-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"o--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nIn this case, I have used sklearn in order to calculate these statistics, but the actual formulas for this are as follows:\nBIC = k ln(n) - 2 ln(L) AIC = 2 k - 2 ln(L)\nL is the maximum value of the likelihood function for the model k is the number of estimated parameters in the model n is the sample size\nBy looking at the minimum point on either of these lines, we can see that a value of four for the number of mixture components would be best. This helps us to reduce the chance of potentially overfitting the model which would make it harder for us to interpret results accurately.\nFor now, we will just use ten for the number of initilizations, but this can be varied to see how the results turn out best later. With this input we can go ahead and fit a matrix to the data and plot the results using the sklearn and matplotlib libraries.\n\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\nGaussianMixture(n_components=4, n_init=10, random_state=27)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=4, n_init=10, random_state=27)\n\n\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nplt.show()\n\n\n\n\nOnce we have fit the data we can go ahead and plot it again along with the centroids and decision boundries the algorithm came up with. Now, we can see that the algorithm did a very good job of fitting to the four distinct clusters. We can see the red dashed lines around each of the more densely packed clusters and the boundry between the two more sparsely packed clusters. From this, we can start analyzing the clusters to see distinct similariets between the groups.\nFor example, if this data was the search habits of different users on an online pharmacy we might be able to see that distinct age ranges are interested in certain products. This might give us the idea to potentially break up products based on age. I we were using some sort of dimendionality reduction then maybe we could see that other factors may impact interest such as weight or gender, but that will have to wait till a later blog post."
  },
  {
    "objectID": "posts/Clustering/clustering.html",
    "href": "posts/Clustering/clustering.html",
    "title": "Clusters with a Gaussian Matrix",
    "section": "",
    "text": "Code\nimport sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\nClustering is a technique used to help us find trends in complex data sets that might be hard to identify manually. With the help of machine learning techniques such as DBScan, KMeans, Gausian Matrix and others we can group the data into clusters that we can then work to interpret. Classifcation is another technique that also breaks up data sets into distinct groups based on different criteria, but with classification there tends to be groups we are setting out to fit the data into. Clustering on the other hand works with data that we have little to no information about before we start working with. With clustering the goal is to look at the clusters and figure out what seperates on cluster from another.\nFor example clustering might be used to help inform an e-commerce site on what they may want to add or adjust based on the tendencies of current customers. They could take the purchase history or viewing history of users and generate clusters. From there, they can analyze them to see if there is specific interest in a certain types of products. From there they could better design their website to meet the needs of their customers.\n\n\nCode\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\n\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\nplt.grid()\nplt.show()\n\n\n\n\n\nTo help demonstrate what the process of using a clustering technique might look like I have generated some data using sklearn’s make_blobs function. This data is completely meaningless, but that with clustering that is completely fine. By looking at the plot of the data we can pretty easily see that there are 4 clusters to consider. Two of them are rather closely packed and two are rather spread out.\nWhen considering what clustering technque there are three that came to mind: KMeans, dbscan, and Gaussian Matrix. KMeans would struggle with this particular data set because two of the clusters are not normal or circular. Dbscan would also struggle due to the rather spread out nature of the other two clusters. Gaussian matrix though should be able to deal with this data set well as it is not anticipating the clusters being circular and not based on the distance seperating one specific data point from another. As a result, I will be using a gaussian matrix to help assign clusters and demonstrate this technique.\nThere are two main criteria that we want to give to the program in order to set up the gaussian matrix: number of mixture components and the number of initilizations to perform. There are other parameters we can set such as the convariance type, but for this particular data set the defaults for this should be good.\nThe number of mixture components should match out expected number of clusters. Given that there are pretty clearly four distinct clusters we could just use four for the number of mixture components. However, we can also plot either the Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) across multiple different values to figure out what this value should be.\n\n\nCode\nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"o-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"o--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\nIn this case, I have used sklearn in order to calculate these statistics, but the actual formulas for this are as follows:\nBIC = k ln(n) - 2 ln(L) AIC = 2 k - 2 ln(L)\nL is the maximum value of the likelihood function for the model k is the number of estimated parameters in the model n is the sample size\nBy looking at the minimum point on either of these lines, we can see that a value of four for the number of mixture components would be best. This helps us to reduce the chance of potentially overfitting the model which would make it harder for us to interpret results accurately.\nFor now, we will just use ten for the number of initilizations, but this can be varied to see how the results turn out best later. With this input we can go ahead and fit a matrix to the data and plot the results using the sklearn and matplotlib libraries.\n\n\nCode\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\n\nGaussianMixture(n_components=4, n_init=10, random_state=27)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=4, n_init=10, random_state=27)\n\n\n\n\nCode\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nplt.show()\n\n\n\n\n\nOnce we have fit the data we can go ahead and plot it again along with the centroids and decision boundaries the algorithm came up with. Now, we can see that the algorithm did a very good job of fitting to the four distinct clusters. We can see the red dashed lines around each of the more densely packed clusters and the boundary between the two more sparsely packed clusters. From this, we can start analyzing the clusters to see distinct similarity between the groups.\nFor example, if this data was the search habits of different users on an online pharmacy we might be able to see that distinct age ranges are interested in certain products. This might give us the idea to potentially break up products based on age. I we were using some sort of dimensional reduction then maybe we could see that other factors may impact interest such as weight or gender, but that will have to wait till a later blog post.\nInformation used to complete this post was taken from: https://github.com/ageron/handson-ml3"
  },
  {
    "objectID": "posts/anomolyDetection/anomolyDetection.html",
    "href": "posts/anomolyDetection/anomolyDetection.html",
    "title": "Anomaly and Outlier Detection with Clustering Techniques",
    "section": "",
    "text": "Clutering and other machine learning techniques are great for helping us to determine trends in data that we can then use to better inform our decisions. This is an extremely useful tool for just about any field that exists as it can greatly speed up processes for workers. However, with any data set there will generally be outliers that cannot be easily explained. This can be due to any number of different things, but we need to be able to identify these as they can make the decision making process more challenging.\nDoing this can be tough as mistakes in anomoly detection can lead to valid data potentially being overlooked in our findings. Depedning on the technique we are using the way to figure out which points are outliers. For example, with dbscan the outliers are determined as part of the process of fitting the data to the model. This is due to the fact that dbscan works by considering the distance between a point and its nearest neighbors. If the next nearest point is within the preset limits then it adds it to the same cluster and determines it’s neighbors until it finds all of the points in that cluster. Once the the algorthim is finished, any points that were to far from the rest are not included in any of the clusters and are then considered anomolies or outliers. With tools like sklearn we can actually get a data set of just these points that we can then work with to determine if they are infact outliers or if the model needs to be adjusted to better include the points. Of course, for some applications this is not acceptable as you cannot garuntee that every point will be included in the analysis and still get accurate results.\nWith other types of methods, the process of determining outliers is a bit more complex. For example, both KMeans and gaussian matrix techniques will include all data points in at least one cluster that they find. This makes them ideal for situations in which we need to be sure that every data point is considered, but we have to come up with other ways to pick out outliers. To demonstrate this, I am going to use the data and model from a previous post about clustering.\n\n\nCode\nimport sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n\nIMAGES_PATH = Path() / \"images\" / \"ForGaussianMatrix\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n    \n    \nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\n\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='+', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\n\nplt.show()\n\n\n\n\n\nThis data has been generated using sklearn’s makeblob function. There are four distinct clusters in this data which I have used a Gaussian matrix to separate from each other. The crosses mark the centroids of each of the distinct clusters. The red dashed lines represent the decision boundaries of each of the clusters in between which each point is considered to be in that cluster.\nIt is pretty clear that some of these points are very distant from their respective cluster or potentially between a few different ones. We will probably want to take note of these or even exclude them from our considerations as we work with the data. The simplest way to do this is to essentially set a threshold beyond which if any points do not meet the threshold they will be considered an outline. This threshold can be determined by many things. If we are working in a production environment or somewhere were the data input could be wrong we might expect a specific percentage of the data to be anomalies or outlines. We can use this expected rate as our threshold.\nWith this in mind, we will first calculate the density or log-likelihood of each sample. Then we will calculate what the threshold value is and apply it to the data to figure out which points do not meet it.\n\n\nCode\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 2)\nanomalies = X[densities &lt; density_threshold]\n\n#| tags: []\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='x')\nplt.ylim(top=5.1)\n\n\nplt.show()\n\n\n\n\n\nWith the points separated, we can use matplot to plot markers for each of the outliers as can be seen above marked by the red “x” marks. By marking these we can see that most of the points do in fact sit far from their respective cluster centroids which makes them less likely to be in a cluster and more likely to be an outlier. Then we could take this data and try to understand where they are coming from. This can give us information about our processes to reduce the chance for errors in the future or maybe understand a different user we are not currently catering to well.\nInformation used to complete this post was taken from: https://github.com/ageron/handson-ml3"
  },
  {
    "objectID": "anomolyDetection.html",
    "href": "anomolyDetection.html",
    "title": "Patrick Finley's Blog",
    "section": "",
    "text": "#Anomoly/Outlier Detection in Clustering\nClutering and other machine learning techniques are great for helping us to determine trends in data that we can then use to better inform our decisions. This is an extremely useful tool for just about any field that exists as it can greatly speed up processes for workers. However, with any data set there will generally be outliers that cannot be easily explained. This can be due to any number of different things, but we need to be able to identify these as they can make the decision making process more challenging.\nDoing this can be tough as mistakes in anomoly detection can lead to valid data potentially being overlooked in our findings. Depedning on the technique we are using the way to figure out which points are outliers. For example, with dbscan the outliers are determined as part of the process of fitting the data to the model. This is due to the fact that dbscan works by considering the distance between a point and its nearest neighbors. If the next nearest point is within the preset limits then it adds it to the same cluster and determines it’s neighbors until it finds all of the points in that cluster. Once the the algorthim is finished, any points that were to far from the rest are not included in any of the clusters and are then considered anomolies or outliers. With tools like sklearn we can actually get a data set of just these points that we can then work with to determine if they are infact outliers or if the model needs to be adjusted to better include the points. Of course, for some applications this is not acceptable as you cannot garuntee that every point will be included in the analysis and still get accurate results.\nWith other types of methods, the process of determining outliers is a bit more complex. For example, both KMeans and gaussian matrix techniques will include all data points in at least one cluster that they find. This makes them ideal for situations in which we need to be sure that every data point is considered, but we have to come up with other ways to pick out outliers. To demonstrate this, I am going to use the data and model from a previous post about clustering.\n\nimport sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n\nIMAGES_PATH = Path() / \"images\" / \"ForGaussianMatrix\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n    \n    \nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\n\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='+', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\n\nplt.show()\n\n\n\n\nThis data has been generated using sklearn’s makeblob function. There are four distinct clusters in this data which I have used a gaussian matrix to seperate from each other. The crosses mark the centroids of each of the distinct clusters. The red dashed lines represent the decision boundaries of each of the clusters in between which each point is considered to be in that cluster.\nIt is pretty clear that some of these points are very distant from their respective cluster or potentially between a few different ones. We will probably want to take note of these or even exclude them from our considerations as we work with the data. The simplest way to do this is to essentially set a threshhold beyond which if any points do not meet the threshold they will be considered an outlier. This threshold can be determined by many things. If we are working in a production environment or somewhere were the data input could be wrong we might expect a specific percentage of the data to be anomolies or outliers. We can use this expected rate as our threshold.\nWith this in mind, we will first calculate the density or log-likelihood of each sample. Then we will calculate what the threshold value is and apply it to the data to figure out which points do not meet it.\n\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 2)\nanomalies = X[densities &lt; density_threshold]\n\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='x')\nplt.ylim(top=5.1)\n\n\nplt.show()\n\n\n\n\nWith the points seperated, we can use matplot to plot markers for each of the outliers as can be seen above marked by the red “x” marks. By marking these we can see that most of the points do in fact sit far from their respective culster centroids which makes them less likely to be in a cluster and more likely to be an outlier. Then we could take this data and try to understand where they are coming from. This can give us information about our processes to reduce the chance for errors in the future or maybe understand a different user we are not currently catering to well."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "My Posts",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/linearAndNonlinear/linearAndNonlinear.html",
    "href": "posts/linearAndNonlinear/linearAndNonlinear.html",
    "title": "Linear and Non-linear Techniques",
    "section": "",
    "text": "Linear regression is one of the oldest and common modeling approaches that exists. In its simplest form the goal is to help relate the change in scalar response between two variables. This can help us understand the relationship two different variable have and potentially understand if certain factors have bigger impacts than others.\nTo help demonstrate this I will use the diabetes data base provided by sklearn. This data base has 10 different variables that are mean centered and scaled by the standard deviation times the square root of n_samples. There is also target variable that tracks the progression of diabetes in the patients over the course of one year. We can use a linear regression to figure out the effect of the 10 different variables on the progression of the disease as seen below.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn import  datasets, linear_model, tree\n\ndef plotDiabetes1(setNumber, xlabel):\n\n    diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n\n    diabetes_X = diabetes_X[:, np.newaxis, setNumber]\n\n    diabetes_X_train = diabetes_X[:-20]\n    diabetes_X_test = diabetes_X[-20:]\n\n    diabetes_y_train = diabetes_y[:-20]\n    diabetes_y_test = diabetes_y[-20:]\n\n    regr = linear_model.LinearRegression()\n\n    regr.fit(diabetes_X_train, diabetes_y_train)\n\n    diabetes_y_pred = regr.predict(diabetes_X_test)\n\n    #ax= plt.subplot(2,2,3)\n    plt.scatter(diabetes_X_test, diabetes_y_test, color=\"black\")\n    plt.plot(diabetes_X_test, diabetes_y_pred, color=\"blue\", linewidth=3)\n\n    plt.title(\"Impact of \" + xlabel)\n    plt.xlabel(\"Standardized \" + xlabel)\n    plt.ylabel(\"target\")\n    plt.grid()\n    plt.show()\n    \n    \n\n    \nbmiPlot = plotDiabetes1(2, \"BMI\")\nbpPlot = plotDiabetes1(3, \"Blood Pressure\")\nagePlot = plotDiabetes1(0, \"Age\")\n\n\n\n\n\n\n\n\n\n\n\nThese graphs are scatters of the test data with an overlay of the regression line on top. From this we can see that there is some trend, but there is a lot of variance. The regression being fit helps us to better understand the relationship. We can see that each of these lines has a positive slope telling us that there is some relationship between the progression and the different factors.\n\n\nCode\ndef plotDiabetes2(setNumber, xlabel, color):\n\n    diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n\n    diabetes_X = diabetes_X[:, np.newaxis, setNumber]\n\n    diabetes_X_train = diabetes_X[:-20]\n    diabetes_X_test = diabetes_X[-20:]\n\n    diabetes_y_train = diabetes_y[:-20]\n    diabetes_y_test = diabetes_y[-20:]\n\n    regr = linear_model.LinearRegression()\n\n    regr.fit(diabetes_X_train, diabetes_y_train)\n\n    diabetes_y_pred = regr.predict(diabetes_X_test)\n\n    ax= plt.subplot(2,2,3)\n    #plt.scatter(diabetes_X_test, diabetes_y_test, color=\"black\")\n    plt.plot(diabetes_X_test, diabetes_y_pred, color= color, linewidth=3, label= xlabel)\n    \n    \n    \n    plt.legend(loc=\"upper left\")\n    plt.title(\"Impact of Different Contributers\")\n    plt.xlabel(\"Standardized Markers\")\n    plt.ylabel(\"target\")\n    plt.grid()\n    \n    \n\n    \nbmiPlot = plotDiabetes2(2, \"BMI\", \"Blue\")\nbpPlot = plotDiabetes2(3, \"BP\", \"Red\")\nagePlot = plotDiabetes2(0, \"Age\", \"Black\")\n\n\n\n\n\nIf we plot the regression lines of the three factors all on one graph we can get an idea of which factors have more impact on the outcomes of the patients. From this though we can tell that each one has some impact we can very clearly see that BMI and blood pressure are better indicators of the progression of the disease over the course of a year. This information can give researchers a better idea of what factors to look into and doctors ideas of what factors they should look for in patients that come to see them.\nA linear regression might work very well for certain applications, but sometimes it is not enough to truly understand the complexity of the problem. So we can also use non-linear approaches to help us with these other problems. A good example of one of these techniques would be a decision tree. While a linear regression is great for a few variables with one specific target outcome, a decision tree can help us classify data into different groups to help demonstrate trends in the data that may be more complex to recognize.\nTo demonstrate this, I will be using the penguins data set provided in Seaborn. The data represents three species of penguins and the general characteristics of 344 penguins of those species. We can look at these characteristics and use them to try and determine if there are any specific parameters that distinguish the different species. A decision tree can help us with this as it will go through the three given parameters (bill length/depth and flipper length) and try to determine any distinct differences to break up the data between species. Due to the fact that a decision tree breaks up its processing into different if/then statements it does not work in a linear fashion like the linear regression does.\n\n\nCode\nimport seaborn as sb\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.tree import plot_tree\n\n\npenguins = sb.load_dataset(\"penguins\")\n\ny= penguins.species\nx= np.array([[penguins.bill_length_mm, penguins.bill_depth_mm, penguins.flipper_length_mm]])\nx = x.reshape(x.shape[1:])\nx = x.transpose()\n\nlen(penguins.species.values)\n\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=27) \n\nclf = DecisionTreeClassifier(max_depth=2, random_state=42)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n\n\n\n\n\n\nplot_tree(\n    clf,\n    filled=True,\n    feature_names=[\"Bill Length (mm)\", \"Bill Depth (mm)\", \"Flipper Length (mm)\"],\n    rounded=True,\n    \n);\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))  \n\n\nAccuracy: 0.9903846153846154\n\n\n\n\n\nAbove you can see the tree and how it made its decision to break up the penguins into three distinct class. You can see that initially it compared the flipper length, then it went to bill depth to try and break apart the data. If we needed we could have the tree go through more branches to try to see if the results could get better. However, we want to be careful to not over fit the data. To tell how well the model is doing we can take a look at the accuracy by comparing the actual results of the test data to the predicted values. From this as seen above, we can see that the accuracy is 99% which is fairly good. With this in mind we would not want to go deeper with the tree, but we can see that the tree has done a great job of helping us analyze this data in a non-linear manner.\nReferences:\nInformation for diabetes data set taken from:\nhttps://rowannicholls.github.io/python/data/sklearn_datasets/diabetes.html\nhttps://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\nInformation for penguins database and decision trees taken from:\nhttps://github.com/ageron/handson-ml3\nhttps://www.datacamp.com/tutorial/decision-tree-classification-python\nhttps://seaborn.pydata.org/archive/0.11/tutorial/function_overview.html"
  },
  {
    "objectID": "posts/classification/classification.html",
    "href": "posts/classification/classification.html",
    "title": "Classification with Decision Trees",
    "section": "",
    "text": "Classification is a very important technique for helping us understand data and use that information in the future. The goal of classification is to take a set of data and break it up into distinct groups based on the characteristics in the data set. Typically, the data is broken up into a testing set and a training set. The training set is used to train the classification model on how to predict what characteristics distinguish between the different groups.\nFor example, one of the most common toy data sets used is the flowers data set. This contains the physical characteristics of three different types of flowers which can then be used to determine what a type a data point may be. An analyst may take the data set and take 70% of the data evenly from the set to train the model. The remaining data can be used later to verify the accuracy of the model once the parameters have been fit. We can use the predicted flower type from the model to compare to the true value and check the accuracy of it.\nThere are many different models we could use to do this process for example: logistic regression, decision trees, support vector machines, random forests, and many more. To help demonstrate this though, I will be using a decision tree classification and the wine data set that is pre-loaded into python with Sklearn.\n\n\nCode\nimport seaborn as sb\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.datasets import load_wine\nfrom sklearn.tree import DecisionTreeClassifier\n\nwine = load_wine(as_frame=True)\nX_wine = wine.data[[\"alcohol\", \"ash\"]].values\ny_wine = wine.target\n\nX_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, test_size=0.3, random_state=27) \ntree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf.fit(X_train, y_train)\ny_pred = tree_clf.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\nplot_tree(\n    tree_clf,\n    filled=True,\n    feature_names=[\"Alcohol\", \"Ash\"],\n    rounded=True,\n    \n);\n\n\nAccuracy: 0.6296296296296297\n\n\n\n\n\nAs you can see above, the decision tree takes the set and starts from the top breaking up the data based on different decisions the model makes. Once it has broken up the wines based on alcohol it moves on to break it up further based on the ash characteristic. If we let it the tree could go through another level of decision making to try and try to split up the data better. However, it is a good idea to take a look at the accuracy of the model to see if ash and alcohol are good determining characteristics. The accuracy score is 62% which is better than just randomly guessing, but not great.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\nplt.figure(figsize=(8, 4))\n\nlengths, widths = np.meshgrid(np.linspace(0, 7.2, 100), np.linspace(0, 3, 100))\nX_wine_all = np.c_[lengths.ravel(), widths.ravel()]\ny_pred = tree_clf.predict(X_wine_all).reshape(lengths.shape)\n\nfor idx, (name, style) in enumerate(zip(wine.target_names, (\"yo\", \"bs\", \"g^\"))):\n    plt.plot(X_wine[:, 0][y_wine == idx], X_wine[:, 1][y_wine == idx],\n             style, label=f\"wine {name}\")\n\n\ntree_clf_deeper = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_clf_deeper.fit(X_wine, y_wine)\n\nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Ash\")\n\nplt.legend()\n\n\nplt.show()\n\n\n\n\n\nIf we make a scatter plot of the data using the alcohol and ash information we can see that the alcohol does seem to split up the data some. However, the ash seems to really have no impact on the data at all. Based on this we need to find a different secondary characteristic to help us break up the data.\nWe can try making another tree using the hue characteristic instead of ash.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nwine = load_wine(as_frame=True)\nX_wine2 = wine.data[[\"alcohol\", \"hue\"]].values\ny_wine2 = wine.target\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_wine2, y_wine2, test_size=0.3, random_state=27) \ntree_clf2 = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf2.fit(X_train2, y_train2)\ny_pred2 = tree_clf2.predict(X_test2)\n\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\nplt.figure(figsize=(8, 4))\n\nlengths, widths = np.meshgrid(np.linspace(0, 7.2, 100), np.linspace(0, 3, 100))\nX_wine_all = np.c_[lengths.ravel(), widths.ravel()]\ny_pred2 = tree_clf2.predict(X_wine_all).reshape(lengths.shape)\n\nfor idx, (name, style) in enumerate(zip(wine.target_names, (\"yo\", \"bs\", \"g^\"))):\n    plt.plot(X_wine2[:, 0][y_wine2 == idx], X_wine2[:, 1][y_wine2 == idx],\n             style, label=f\"wine {name}\")\n\n\ntree_clf_deeper = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_clf_deeper.fit(X_wine2, y_wine2)\n\nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Hue\")\n\nplt.legend()\n\n\nplt.show()\n\n\n\n\n\nThe above plot is another scatter plot, but this time with the hue plotted against it. From this we can see that the hue clearly plays a roll in separating the data, so the decision tree should be able to better break up the set.\n\n\nCode\nwine = load_wine(as_frame=True)\nX_wine2 = wine.data[[\"alcohol\", \"hue\"]].values\ny_wine2 = wine.target\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_wine2, y_wine2, test_size=0.3, random_state=27) \ntree_clf2 = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf2.fit(X_train2, y_train2)\ny_pred2 = tree_clf2.predict(X_test2)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test2, y_pred2))\n\nplot_tree(\n    tree_clf2,\n    filled=True,\n    feature_names=[\"Alcohol\", \"Hue\"],\n    rounded=True,\n    \n);\n\n\nAccuracy: 0.9074074074074074\n\n\n\n\n\nThe above tree was generated using the hue and alcohol content of the wines. You can see that at the first step the wines were broken up by alcohol which mostly broke up classes 0 and 1. Then at the second step the data was broken up by hue which pulled out most of class 2. This can be seen with the accuracy which is now at 90%. This is significantly higher than the previous attempt which proves that the hue was the correct call for this data set.\nThis decision tree tells us that the data can be effectively broken up by hue and alcohol, but there are other characteristics we could also try to help us break up the data if we wanted to work on getting the accuracy higher. We could also try different models to see if one might fit the data a bit better due to how they work through the data. These techniques can be used to help researchers better understand the realities around us every day much more quickly then by traditional manual work.\nInformation used to complete this post was taken from:\nhttps://github.com/ageron/handson-ml3\nhttps://scikit-learn.org/stable/datasets/toy_dataset.html"
  },
  {
    "objectID": "posts/classification.html",
    "href": "posts/classification.html",
    "title": "Classification with Decision Trees",
    "section": "",
    "text": "Classification is a very important technique for helping us understand data and use that information in the future. The goal of classification is to take a set of data and break it up into distinct groups based on the characteristics in the data set. Typically, the data is broken up into a testing set and a training set. The training set is used to train the classification model on how to predict what characteristics distinguish between the different groups.\nFor example, one of the most common toy data sets used is the flowers data set. This contains the physical characteristics of three different types of flowers which can then be used to determine what a type a data point may be. A analyist may take the data set and take 70% of the data evenly from the set to train the model. The remaining data can be used later to verify the accuracy of the model once the parameters have been fit. We can use the predicted flower type from the model to compare to the true value and check the accuracy of it.\nThere are many different models we could use to do this process for example: logistic regression, decision trees, support vector machines, random forests, and many more. To help demonstrate this though, I will be using a decision tree classification and the wine data set that is preloaded into python with Sklearn.\n\nimport seaborn as sb\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.datasets import load_wine\nfrom sklearn.tree import DecisionTreeClassifier\n\nwine = load_wine(as_frame=True)\nX_wine = wine.data[[\"alcohol\", \"ash\"]].values\ny_wine = wine.target\n\nX_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, test_size=0.3, random_state=27) \ntree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf.fit(X_train, y_train)\ny_pred = tree_clf.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\nplot_tree(\n    tree_clf,\n    filled=True,\n    feature_names=[\"Alcohol\", \"Ash\"],\n    rounded=True,\n    \n);\n\nAccuracy: 0.6296296296296297\n\n\n\n\n\nAs you can see above, the decision tree takes the set and starts from the top breaking up the data based on different decisions the model makes. Once it has broken up the wines based on alcohol it moves on to break it up further based on the ash characteristic. If we let it the tree could go through another level of decision making to try and try to split up the data better. However, it is a good idea to take a look at the accuracy of the model to see if ash and alcohol are good determining characteristics. The accuracy score is 62% which is better than just randomly guessing, but not great.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\nplt.figure(figsize=(8, 4))\n\nlengths, widths = np.meshgrid(np.linspace(0, 7.2, 100), np.linspace(0, 3, 100))\nX_wine_all = np.c_[lengths.ravel(), widths.ravel()]\ny_pred = tree_clf.predict(X_wine_all).reshape(lengths.shape)\n\nfor idx, (name, style) in enumerate(zip(wine.target_names, (\"yo\", \"bs\", \"g^\"))):\n    plt.plot(X_wine[:, 0][y_wine == idx], X_wine[:, 1][y_wine == idx],\n             style, label=f\"wine {name}\")\n\n\ntree_clf_deeper = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_clf_deeper.fit(X_wine, y_wine)\n\nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Ash\")\n\nplt.legend()\n\n\nplt.show()\n\n\n\n\nIf we make a scatter plot of the data using the alcohol and ash information we can see that the alcohol does seem to split up the data some. However, the ash seems to really have no impact on the data at all. Based on this we need to find a different secondary characteristic to help us break up the data.\nWe can try making another tree using the hue characteristic instead of ash.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\nplt.figure(figsize=(8, 4))\n\nlengths, widths = np.meshgrid(np.linspace(0, 7.2, 100), np.linspace(0, 3, 100))\nX_wine_all = np.c_[lengths.ravel(), widths.ravel()]\ny_pred2 = tree_clf2.predict(X_wine_all).reshape(lengths.shape)\n\nfor idx, (name, style) in enumerate(zip(wine.target_names, (\"yo\", \"bs\", \"g^\"))):\n    plt.plot(X_wine2[:, 0][y_wine2 == idx], X_wine2[:, 1][y_wine2 == idx],\n             style, label=f\"wine {name}\")\n\n\ntree_clf_deeper = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_clf_deeper.fit(X_wine2, y_wine2)\n\nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Hue\")\n\nplt.legend()\n\n\nplt.show()\n\n\n\n\nThe above plot is another scatter plot, but this time with the hue plotted against it. From this we can see that the hue clearly plays a roll in seperating the data, so the decision tree should be able to better break up the set.\n\nwine = load_wine(as_frame=True)\nX_wine2 = wine.data[[\"alcohol\", \"hue\"]].values\ny_wine2 = wine.target\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_wine2, y_wine2, test_size=0.3, random_state=27) \ntree_clf2 = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf2.fit(X_train2, y_train2)\ny_pred2 = tree_clf2.predict(X_test2)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test2, y_pred2))\n\nplot_tree(\n    tree_clf2,\n    filled=True,\n    feature_names=[\"Alcohol\", \"Hue\"],\n    rounded=True,\n    \n);\n\nAccuracy: 0.9074074074074074\n\n\n\n\n\nThe above tree was generated using the hue and alcohol content of the wines. You can see that at the first step the wines were broken up by alcohol which mostly broke up classes 0 and 1. Then at the second step the data was broken up by hue which pulled out most of class 2. This can be seen with the accuracy which is now at 90%. This is significantly higher than the previous attempt which proves that the hue was the correct call for this data set.\nThis decision tree tells us that the data can be effectively broken up by hue and alcohol, but there are other characteristics we could also try to help us break up the data if we wanted to work on getting the accuracy higher. We could also try different models to see if one might fit the data a bit better due to how they work through the data. These techniques can be used to help researchers better understand the realities around us every day much more quickly then by traditional manual work."
  },
  {
    "objectID": "posts/clustering.html",
    "href": "posts/clustering.html",
    "title": "Clusters with a Gaussian Matrix",
    "section": "",
    "text": "import sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\nClustering is a technique used to help us find trends in complex data sets that might be hard to identify manually. With the help of machine learning techniques such as DBScan, KMeans, Gausian Matrix and others we can group the data into clusters that we can then work to interpret. Classifcation is another technique that also breaks up data sets into distinct groups based on different criteria, but with classification there tends to be groups we are setting out to fit the data into. Clustering on the other hand works with data that we have little to no information about before we start working with. With clustering the goal is to look at the clusters and figure out what seperates on cluster from another.\nFor example clustering might be used to help inform an e-commerce site on what they may want to add or adjust based on the tendencies of current customers. They could take the purchase history or viewing history of users and generate clusters. From there, they can analyze them to see if there is specific interest in a certain types of products. From there they could better design their website to meet the needs of their customers.\n\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\n\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\nplt.grid()\nplt.show()\n\n\n\n\nTo help demonstrate what the process of using a clustering technique might look like I have generated some data using sklearn’s make_blobs function. This data is completely meaningless, but that with clustering that is completely fine. By looking at the plot of the data we can pretty easily see that there are 4 clusters to consider. Two of them are rather closely packed and two are rather spread out.\nWhen considering what clustering technque there are three that came to mind: KMeans, dbscan, and Gaussian Matrix. KMeans would struggle with this particular data set because two of the clusters are not normal or circular. Dbscan would also struggle due to the rather spread out nature of the other two clusters. Gaussian matrix though should be able to deal with this data set well as it is not anticipating the clusters being circular and not based on the distance seperating one specific data point from another. As a result, I will be using a gaussian matrix to help assign clusters and demonstrate this technique.\nThere are two main criteria that we want to give to the program in order to set up the gaussian matrix: number of mixture components and the number of initilizations to perform. There are other parameters we can set such as the convariance type, but for this particular data set the defaults for this should be good.\nThe number of mixture components should match out expected number of clusters. Given that there are pretty clearly four distinct clusters we could just use four for the number of mixture components. However, we can also plot either the Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) across multiple different values to figure out what this value should be. ********* MORE HERE FOR THE CALCS ************\n\nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"o-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"o--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nIn this case, I have used sklearn in order to calculate these statistics, but the actual formulas for this are as follows:\nBIC = k ln(n) - 2 ln(L) AIC = 2 k - 2 ln(L)\nL is the maximum value of the likelihood function for the model k is the number of estimated parameters in the model n is the sample size\nBy looking at the minimum point on either of these lines, we can see that a value of four for the number of mixture components would be best. This helps us to reduce the chance of potentially overfitting the model which would make it harder for us to interpret results accurately.\nFor now, we will just use ten for the number of initilizations, but this can be varied to see how the results turn out best later. With this input we can go ahead and fit a matrix to the data and plot the results using the sklearn and matplotlib libraries.\n\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\nGaussianMixture(n_components=4, n_init=10, random_state=27)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=4, n_init=10, random_state=27)\n\n\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nplt.show()\n\n\n\n\nOnce we have fit the data we can go ahead and plot it again along with the centroids and decision boundries the algorithm came up with. Now, we can see that the algorithm did a very good job of fitting to the four distinct clusters. We can see the red dashed lines around each of the more densely packed clusters and the boundry between the two more sparsely packed clusters. From this, we can start analyzing the clusters to see distinct similariets between the groups.\nFor example, if this data was the search habits of different users on an online pharmacy we might be able to see that distinct age ranges are interested in certain products. This might give us the idea to potentially break up products based on age. I we were using some sort of dimendionality reduction then maybe we could see that other factors may impact interest such as weight or gender, but that will have to wait till a later blog post."
  },
  {
    "objectID": "posts/clustering/clustering.html",
    "href": "posts/clustering/clustering.html",
    "title": "Clusters with a Gaussian Matrix",
    "section": "",
    "text": "Code\nimport sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\nClustering is a technique used to help us find trends in complex data sets that might be hard to identify manually. With the help of machine learning techniques such as DBScan, KMeans, Gausian Matrix and others we can group the data into clusters that we can then work to interpret. Classifcation is another technique that also breaks up data sets into distinct groups based on different criteria, but with classification there tends to be groups we are setting out to fit the data into. Clustering on the other hand works with data that we have little to no information about before we start working with. With clustering the goal is to look at the clusters and figure out what seperates on cluster from another.\nFor example clustering might be used to help inform an e-commerce site on what they may want to add or adjust based on the tendencies of current customers. They could take the purchase history or viewing history of users and generate clusters. From there, they can analyze them to see if there is specific interest in a certain types of products. From there they could better design their website to meet the needs of their customers.\n\n\nCode\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\n\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\nplt.grid()\nplt.show()\n\n\n\n\n\nTo help demonstrate what the process of using a clustering technique might look like I have generated some data using sklearn’s make_blobs function. This data is completely meaningless, but that with clustering that is completely fine. By looking at the plot of the data we can pretty easily see that there are 4 clusters to consider. Two of them are rather closely packed and two are rather spread out.\nWhen considering what clustering technque there are three that came to mind: KMeans, dbscan, and Gaussian Matrix. KMeans would struggle with this particular data set because two of the clusters are not normal or circular. Dbscan would also struggle due to the rather spread out nature of the other two clusters. Gaussian matrix though should be able to deal with this data set well as it is not anticipating the clusters being circular and not based on the distance seperating one specific data point from another. As a result, I will be using a gaussian matrix to help assign clusters and demonstrate this technique.\nThere are two main criteria that we want to give to the program in order to set up the gaussian matrix: number of mixture components and the number of initilizations to perform. There are other parameters we can set such as the convariance type, but for this particular data set the defaults for this should be good.\nThe number of mixture components should match out expected number of clusters. Given that there are pretty clearly four distinct clusters we could just use four for the number of mixture components. However, we can also plot either the Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) across multiple different values to figure out what this value should be. ********* MORE HERE FOR THE CALCS ************\n\n\nCode\nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"o-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"o--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\nIn this case, I have used sklearn in order to calculate these statistics, but the actual formulas for this are as follows:\nBIC = k ln(n) - 2 ln(L) AIC = 2 k - 2 ln(L)\nL is the maximum value of the likelihood function for the model k is the number of estimated parameters in the model n is the sample size\nBy looking at the minimum point on either of these lines, we can see that a value of four for the number of mixture components would be best. This helps us to reduce the chance of potentially overfitting the model which would make it harder for us to interpret results accurately.\nFor now, we will just use ten for the number of initilizations, but this can be varied to see how the results turn out best later. With this input we can go ahead and fit a matrix to the data and plot the results using the sklearn and matplotlib libraries.\n\n\nCode\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\n\nGaussianMixture(n_components=4, n_init=10, random_state=27)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=4, n_init=10, random_state=27)\n\n\n\n\nCode\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nplt.show()\n\n\n\n\n\nOnce we have fit the data we can go ahead and plot it again along with the centroids and decision boundries the algorithm came up with. Now, we can see that the algorithm did a very good job of fitting to the four distinct clusters. We can see the red dashed lines around each of the more densely packed clusters and the boundry between the two more sparsely packed clusters. From this, we can start analyzing the clusters to see distinct similariets between the groups.\nFor example, if this data was the search habits of different users on an online pharmacy we might be able to see that distinct age ranges are interested in certain products. This might give us the idea to potentially break up products based on age. I we were using some sort of dimendionality reduction then maybe we could see that other factors may impact interest such as weight or gender, but that will have to wait till a later blog post.\nInformation used to complete this post was taken from: https://github.com/ageron/handson-ml3"
  }
]