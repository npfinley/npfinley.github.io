[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "Clusters with a Gaussian Matrix\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Clustering/Clustering.html",
    "href": "posts/Clustering/Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "hello I am testing how dumb I am\n\n\nCode\nimport sklearn\n\n\nIf I type some more down here how does that go"
  },
  {
    "objectID": "posts/Clustering/Clustering.html#if-i-type-some-more-down-here-how-does-that-go",
    "href": "posts/Clustering/Clustering.html#if-i-type-some-more-down-here-how-does-that-go",
    "title": "Clustering",
    "section": "IF I type some more down here how does that go",
    "text": "IF I type some more down here how does that go"
  },
  {
    "objectID": "GaussianMatrix.html",
    "href": "GaussianMatrix.html",
    "title": "Patrick Finley's Blog",
    "section": "",
    "text": "import sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n\nfrom pathlib import Path\n\nIMAGES_PATH = Path() / \"images\" / \"ForGaussianMatrix\"\nIMAGES_PATH.mkdir(parents=True, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n\n\nimport os\nos.environ[\"MKL_NUM_THREADS\"] = \"1\" \nos.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" \nos.environ[\"OMP_NUM_THREADS\"] = \"6\" \n\n\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\nplt.grid()\nsave_fig(\"blobs_plot\")\nplt.show()\n\n\n\n\n\nfrom sklearn.mixture import GaussianMixture\n\n\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\nGaussianMixture(n_components=4, n_init=10, random_state=27)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=4, n_init=10, random_state=27)\n\n\n\ngm.weights_\n\narray([0.19838075, 0.20086554, 0.30224528, 0.29850842])\n\n\n\ngm.means_\n\narray([[-2.46484766,  1.28737693],\n       [ 3.68999774,  2.36417118],\n       [-1.20619237, -1.17594382],\n       [ 1.21207915,  3.59365466]])\n\n\n\ngm.covariances_\n\narray([[[1.08547535, 0.07669351],\n        [0.07669351, 0.93922256]],\n\n       [[1.07142397, 0.02135635],\n        [0.02135635, 1.15576698]],\n\n       [[0.17377385, 0.14322807],\n        [0.14322807, 0.18633513]],\n\n       [[0.2210167 , 0.17509654],\n        [0.17509654, 0.21530691]]])\n\n\n\ngm.converged_\n\nTrue\n\n\n\ngm.n_iter_\n\n4\n\n\n\ngm.predict(X)\n\narray([3, 3, 3, ..., 1, 1, 1], dtype=int64)\n\n\n\ngm.predict_proba(X).round(3)\n\narray([[0.   , 0.003, 0.   , 0.997],\n       [0.   , 0.466, 0.   , 0.534],\n       [0.001, 0.019, 0.   , 0.979],\n       ...,\n       [0.   , 1.   , 0.   , 0.   ],\n       [0.   , 1.   , 0.   , 0.   ],\n       [0.   , 1.   , 0.   , 0.   ]])\n\n\n\nX_new, y_new = gm.sample(6)\nX_new\n\narray([[-2.52953419,  2.26392006],\n       [ 3.04258093,  3.26937155],\n       [ 4.20922236,  3.09372466],\n       [-1.45605325, -1.26906405],\n       [ 1.27432055,  3.73269449],\n       [ 1.91022481,  3.84924244]])\n\n\n\ny_new\n\narray([0, 1, 1, 2, 3, 3])\n\n\n\ngm.score_samples(X).round(2)\n\narray([-1.28, -4.82, -3.2 , ..., -3.84, -3.82, -3.77])\n\n\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z,\n                 norm=LogNorm(vmin=1.0, vmax=30.0),\n                 levels=np.logspace(0, 2, 12))\n    plt.contour(xx, yy, Z,\n                norm=LogNorm(vmin=1.0, vmax=30.0),\n                levels=np.logspace(0, 2, 12),\n                linewidths=1, colors='k')\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nsave_fig(\"gaussian_mixtures_plot\")\nplt.show()\n\n\n\n\n\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"bo-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"go--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.legend()\nplt.grid()\nsave_fig(\"aic_bic_vs_k_plot\")\nplt.show()\n\n\n\n\n\n\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 2)\nanomalies = X[densities &lt; density_threshold]\n\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\nplt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')\nplt.ylim(top=5.1)\n\nsave_fig(\"mixture_anomaly_detection_plot\")\nplt.show()"
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clusters with a Gaussian Matrix",
    "section": "",
    "text": "import sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\nClustering is a technique used to help us find trends in complex data sets that might be hard to identify manually. With the help of machine learning techniques such as DBScan, KMeans, Gausian Matrix and others we can group the data into clusters that we can then work to interpret. Classifcation is another technique that also breaks up data sets into distinct groups based on different criteria, but with classification there tends to be groups we are setting out to fit the data into. Clustering on the other hand works with data that we have little to no information about before we start working with. With clustering the goal is to look at the clusters and figure out what seperates on cluster from another.\nFor example clustering might be used to help inform an e-commerce site on what they may want to add or adjust based on the tendencies of current customers. They could take the purchase history or viewing history of users and generate clusters. From there, they can analyze them to see if there is specific interest in a certain types of products. From there they could better design their website to meet the needs of their customers.\n\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\n\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\nplt.grid()\nplt.show()\n\n\n\n\nTo help demonstrate what the process of using a clustering technique might look like I have generated some data using sklearn’s make_blobs function. This data is completely meaningless, but that with clustering that is completely fine. By looking at the plot of the data we can pretty easily see that there are 4 clusters to consider. Two of them are rather closely packed and two are rather spread out.\nWhen considering what clustering technque there are three that came to mind: KMeans, dbscan, and Gaussian Matrix. KMeans would struggle with this particular data set because two of the clusters are not normal or circular. Dbscan would also struggle due to the rather spread out nature of the other two clusters. Gaussian matrix though should be able to deal with this data set well as it is not anticipating the clusters being circular and not based on the distance seperating one specific data point from another. As a result, I will be using a gaussian matrix to help assign clusters and demonstrate this technique.\nThere are two main criteria that we want to give to the program in order to set up the gaussian matrix: number of mixture components and the number of initilizations to perform. There are other parameters we can set such as the convariance type, but for this particular data set the defaults for this should be good.\nThe number of mixture components should match out expected number of clusters. Given that there are pretty clearly four distinct clusters we could just use four for the number of mixture components. However, we can also plot either the Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) across multiple different values to figure out what this value should be. ********* MORE HERE FOR THE CALCS ************\n\nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"o-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"o--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\nIn this case, I have used sklearn in order to calculate these statistics, but the actual formulas for this are as follows:\nBIC = k ln(n) - 2 ln(L) AIC = 2 k - 2 ln(L)\nL is the maximum value of the likelihood function for the model k is the number of estimated parameters in the model n is the sample size\nBy looking at the minimum point on either of these lines, we can see that a value of four for the number of mixture components would be best. This helps us to reduce the chance of potentially overfitting the model which would make it harder for us to interpret results accurately.\nFor now, we will just use ten for the number of initilizations, but this can be varied to see how the results turn out best later. With this input we can go ahead and fit a matrix to the data and plot the results using the sklearn and matplotlib libraries.\n\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\nGaussianMixture(n_components=4, n_init=10, random_state=27)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=4, n_init=10, random_state=27)\n\n\n\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nplt.show()\n\n\n\n\nOnce we have fit the data we can go ahead and plot it again along with the centroids and decision boundries the algorithm came up with. Now, we can see that the algorithm did a very good job of fitting to the four distinct clusters. We can see the red dashed lines around each of the more densely packed clusters and the boundry between the two more sparsely packed clusters. From this, we can start analyzing the clusters to see distinct similariets between the groups.\nFor example, if this data was the search habits of different users on an online pharmacy we might be able to see that distinct age ranges are interested in certain products. This might give us the idea to potentially break up products based on age. I we were using some sort of dimendionality reduction then maybe we could see that other factors may impact interest such as weight or gender, but that will have to wait till a later blog post."
  },
  {
    "objectID": "posts/Clustering/clustering.html",
    "href": "posts/Clustering/clustering.html",
    "title": "Clusters with a Gaussian Matrix",
    "section": "",
    "text": "Code\nimport sys\n\nassert sys.version_info &gt;= (3, 7)\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nfrom pathlib import Path\n    \nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\n\nClustering is a technique used to help us find trends in complex data sets that might be hard to identify manually. With the help of machine learning techniques such as DBScan, KMeans, Gausian Matrix and others we can group the data into clusters that we can then work to interpret. Classifcation is another technique that also breaks up data sets into distinct groups based on different criteria, but with classification there tends to be groups we are setting out to fit the data into. Clustering on the other hand works with data that we have little to no information about before we start working with. With clustering the goal is to look at the clusters and figure out what seperates on cluster from another.\nFor example clustering might be used to help inform an e-commerce site on what they may want to add or adjust based on the tendencies of current customers. They could take the purchase history or viewing history of users and generate clusters. From there, they can analyze them to see if there is specific interest in a certain types of products. From there they could better design their website to meet the needs of their customers.\n\n\nCode\nX1, y1 = make_blobs(n_samples=750, centers=((-2, 10), (-2, -2)), random_state=27)\n\nX1 = X1.dot(np.array([[0.4, 0.2], [0.2, 0.4]]))\nX2, y2 = make_blobs(n_samples=500, centers=2, random_state=27)\nX2 = X2 + [-1, -5]\nX = np.r_[X1, X2]\ny = np.r_[y1, y2]\n\n\ndef plot_clusters(X, y=None):\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\", rotation=0)\n\nplt.figure(figsize=(8, 4))\nplot_clusters(X)\nplt.gca().set_axisbelow(True)\nplt.grid()\nplt.show()\n\n\n\n\n\nTo help demonstrate what the process of using a clustering technique might look like I have generated some data using sklearn’s make_blobs function. This data is completely meaningless, but that with clustering that is completely fine. By looking at the plot of the data we can pretty easily see that there are 4 clusters to consider. Two of them are rather closely packed and two are rather spread out.\nWhen considering what clustering technque there are three that came to mind: KMeans, dbscan, and Gaussian Matrix. KMeans would struggle with this particular data set because two of the clusters are not normal or circular. Dbscan would also struggle due to the rather spread out nature of the other two clusters. Gaussian matrix though should be able to deal with this data set well as it is not anticipating the clusters being circular and not based on the distance seperating one specific data point from another. As a result, I will be using a gaussian matrix to help assign clusters and demonstrate this technique.\nThere are two main criteria that we want to give to the program in order to set up the gaussian matrix: number of mixture components and the number of initilizations to perform. There are other parameters we can set such as the convariance type, but for this particular data set the defaults for this should be good.\nThe number of mixture components should match out expected number of clusters. Given that there are pretty clearly four distinct clusters we could just use four for the number of mixture components. However, we can also plot either the Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) across multiple different values to figure out what this value should be.\n\n\nCode\nfrom sklearn.mixture import GaussianMixture\ngms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X)\n             for k in range(1, 11)]\nbics = [model.bic(X) for model in gms_per_k]\naics = [model.aic(X) for model in gms_per_k]\n\nplt.figure(figsize=(8, 3))\nplt.plot(range(1, 11), bics, \"o-\", label=\"BIC\")\nplt.plot(range(1, 11), aics, \"o--\", label=\"AIC\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Information Criterion\")\nplt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\nIn this case, I have used sklearn in order to calculate these statistics, but the actual formulas for this are as follows:\nBIC = k ln(n) - 2 ln(L) AIC = 2 k - 2 ln(L)\nL is the maximum value of the likelihood function for the model k is the number of estimated parameters in the model n is the sample size\nBy looking at the minimum point on either of these lines, we can see that a value of four for the number of mixture components would be best. This helps us to reduce the chance of potentially overfitting the model which would make it harder for us to interpret results accurately.\nFor now, we will just use ten for the number of initilizations, but this can be varied to see how the results turn out best later. With this input we can go ahead and fit a matrix to the data and plot the results using the sklearn and matplotlib libraries.\n\n\nCode\ngm = GaussianMixture(n_components=4, n_init=10, random_state=27)\ngm.fit(X)\n\n\nGaussianMixture(n_components=4, n_init=10, random_state=27)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixtureGaussianMixture(n_components=4, n_init=10, random_state=27)\n\n\n\n\nCode\nfrom matplotlib.colors import LogNorm\n\ndef plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n    if weights is not None:\n        centroids = centroids[weights &gt; weights.max() / 10]\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='o', s=35, linewidths=8,\n                color=circle_color, zorder=10, alpha=0.9)\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', s=2, linewidths=12,\n                color=cross_color, zorder=11, alpha=1)\n\ndef plot_gaussian_mixture(clusterer, X, resolution=1000, show_ylabels=True):\n    mins = X.min(axis=0) - 0.1\n    maxs = X.max(axis=0) + 0.1\n    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n                         np.linspace(mins[1], maxs[1], resolution))\n    Z = -clusterer.score_samples(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z,\n                linewidths=2, colors='r', linestyles='dashed')\n    \n    plt.plot(X[:, 0], X[:, 1], '.', markersize=3)\n    plot_centroids(clusterer.means_, clusterer.weights_)\n\n    plt.xlabel(\"$x_1$\")\n    if show_ylabels:\n        plt.ylabel(\"$x_2$\", rotation=0)\n    else:\n        plt.tick_params(labelleft=False)\n\nplt.figure(figsize=(8, 4))\n\nplot_gaussian_mixture(gm, X)\n\nplt.show()\n\n\n\n\n\nOnce we have fit the data we can go ahead and plot it again along with the centroids and decision boundries the algorithm came up with. Now, we can see that the algorithm did a very good job of fitting to the four distinct clusters. We can see the red dashed lines around each of the more densely packed clusters and the boundry between the two more sparsely packed clusters. From this, we can start analyzing the clusters to see distinct similariets between the groups.\nFor example, if this data was the search habits of different users on an online pharmacy we might be able to see that distinct age ranges are interested in certain products. This might give us the idea to potentially break up products based on age. I we were using some sort of dimendionality reduction then maybe we could see that other factors may impact interest such as weight or gender, but that will have to wait till a later blog post."
  }
]