{
  "hash": "11ab78a7771b5b34e797d399364bc5b7",
  "result": {
    "markdown": "---\ntitle: Classification with Decision Trees\nformat:\n  html:\n    code-fold: true\n---\n\nClassification is a very important technique for helping us understand data and use that information in the future. The goal of classification is to take a set of data and break it up into distinct groups based on the characteristics in the data set. Typically, the data is broken up into a testing set and a training set. The training set is used to train the classification model on how to predict what characteristics distinguish between the different groups.\n\nFor example, one of the most common toy data sets used is the flowers data set. This contains the physical characteristics of three different types of flowers which can then be used to determine what a type a data point may be. An analyst may take the data set and take 70% of the data evenly from the set to train the model. The remaining data can be used later to verify the accuracy of the model once the parameters have been fit. We can use the predicted flower type from the model to compare to the true value and check the accuracy of it.\n\nThere are many different models we could use to do this process for example: logistic regression, decision trees, support vector machines, random forests, and many more. To help demonstrate this though, I will be using a decision tree classification and the wine data set that is pre-loaded into python with Sklearn.\n\n::: {.cell tags='[]' execution_count=1}\n``` {.python .cell-code}\nimport seaborn as sb\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.datasets import load_wine\nfrom sklearn.tree import DecisionTreeClassifier\n\nwine = load_wine(as_frame=True)\nX_wine = wine.data[[\"alcohol\", \"ash\"]].values\ny_wine = wine.target\n\nX_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, test_size=0.3, random_state=27) \ntree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf.fit(X_train, y_train)\ny_pred = tree_clf.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\nplot_tree(\n    tree_clf,\n    filled=True,\n    feature_names=[\"Alcohol\", \"Ash\"],\n    rounded=True,\n    \n);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.6296296296296297\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](classification_files/figure-html/cell-2-output-2.png){width=540 height=389}\n:::\n:::\n\n\nAs you can see above, the decision tree takes the set and starts from the top breaking up the data based on different decisions the model makes. Once it has broken up the wines based on alcohol it moves on to break it up further based on the ash characteristic. If we let it the tree could go through another level of decision making to try and try to split up the data better. However, it is a good idea to take a look at the accuracy of the model to see if ash and alcohol are good determining characteristics. The accuracy score is 62% which is better than just randomly guessing, but not great.\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\nplt.figure(figsize=(8, 4))\n\nlengths, widths = np.meshgrid(np.linspace(0, 7.2, 100), np.linspace(0, 3, 100))\nX_wine_all = np.c_[lengths.ravel(), widths.ravel()]\ny_pred = tree_clf.predict(X_wine_all).reshape(lengths.shape)\n\nfor idx, (name, style) in enumerate(zip(wine.target_names, (\"yo\", \"bs\", \"g^\"))):\n    plt.plot(X_wine[:, 0][y_wine == idx], X_wine[:, 1][y_wine == idx],\n             style, label=f\"wine {name}\")\n\n\ntree_clf_deeper = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_clf_deeper.fit(X_wine, y_wine)\n\nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Ash\")\n\nplt.legend()\n\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](classification_files/figure-html/cell-3-output-1.png){width=684 height=356}\n:::\n:::\n\n\nIf we make a scatter plot of the data using the alcohol and ash information we can see that the alcohol does seem to split up the data some. However, the ash seems to really have no impact on the data at all. Based on this we need to find a different secondary characteristic to help us break up the data.\n\nWe can try making another tree using the hue characteristic instead of ash.\n\n::: {.cell tags='[]' execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nwine = load_wine(as_frame=True)\nX_wine2 = wine.data[[\"alcohol\", \"hue\"]].values\ny_wine2 = wine.target\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_wine2, y_wine2, test_size=0.3, random_state=27) \ntree_clf2 = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf2.fit(X_train2, y_train2)\ny_pred2 = tree_clf2.predict(X_test2)\n\nfrom matplotlib.colors import ListedColormap\ncustom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\nplt.figure(figsize=(8, 4))\n\nlengths, widths = np.meshgrid(np.linspace(0, 7.2, 100), np.linspace(0, 3, 100))\nX_wine_all = np.c_[lengths.ravel(), widths.ravel()]\ny_pred2 = tree_clf2.predict(X_wine_all).reshape(lengths.shape)\n\nfor idx, (name, style) in enumerate(zip(wine.target_names, (\"yo\", \"bs\", \"g^\"))):\n    plt.plot(X_wine2[:, 0][y_wine2 == idx], X_wine2[:, 1][y_wine2 == idx],\n             style, label=f\"wine {name}\")\n\n\ntree_clf_deeper = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_clf_deeper.fit(X_wine2, y_wine2)\n\nplt.xlabel(\"Alcohol\")\nplt.ylabel(\"Hue\")\n\nplt.legend()\n\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](classification_files/figure-html/cell-4-output-1.png){width=675 height=356}\n:::\n:::\n\n\nThe above plot is another scatter plot, but this time with the hue plotted against it. From this we can see that the hue clearly plays a roll in separating the data, so the decision tree should be able to better break up the set.\n\n::: {.cell tags='[]' execution_count=4}\n``` {.python .cell-code}\nwine = load_wine(as_frame=True)\nX_wine2 = wine.data[[\"alcohol\", \"hue\"]].values\ny_wine2 = wine.target\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_wine2, y_wine2, test_size=0.3, random_state=27) \ntree_clf2 = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf2.fit(X_train2, y_train2)\ny_pred2 = tree_clf2.predict(X_test2)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test2, y_pred2))\n\nplot_tree(\n    tree_clf2,\n    filled=True,\n    feature_names=[\"Alcohol\", \"Hue\"],\n    rounded=True,\n    \n);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.9074074074074074\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](classification_files/figure-html/cell-5-output-2.png){width=540 height=389}\n:::\n:::\n\n\nThe above tree was generated using the hue and alcohol content of the wines. You can see that at the first step the wines were broken up by alcohol which mostly broke up classes 0 and 1. Then at the second step the data was broken up by hue which pulled out most of class 2. This can be seen with the accuracy which is now at 90%. This is significantly higher than the previous attempt which proves that the hue was the correct call for this data set.\n\nThis decision tree tells us that the data can be effectively broken up by hue and alcohol, but there are other characteristics we could also try to help us break up the data if we wanted to work on getting the accuracy higher. We could also try different models to see if one might fit the data a bit better due to how they work through the data. These techniques can be used to help researchers better understand the realities around us every day much more quickly then by traditional manual work.\n\nInformation used to complete this post was taken from:\n\nhttps://github.com/ageron/handson-ml3\n\nhttps://scikit-learn.org/stable/datasets/toy_dataset.html\n\n",
    "supporting": [
      "classification_files"
    ],
    "filters": [],
    "includes": {}
  }
}