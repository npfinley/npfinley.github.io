{
  "hash": "524c46f12167daa2ab519e0305769950",
  "result": {
    "markdown": "---\ntitle: Linear and Non-linear Techniques\nformat:\n  html:\n    code-fold: true\n---\n\nLinear regression is one of the oldest and common modeling approaches that exists. In its simplest form the goal is to help relate the change in scalar response between two variables. This can help us understand the relationship two different variable have and potentially understand if certain factors have bigger impacts than others.\n\nTo help demonstrate this I will use the diabetes data base provided by sklearn. This data base has 10 different variables that are mean centered and scaled by the standard deviation times the square root of n_samples. There is also target variable that tracks the progression of diabetes in the patients over the course of one year. We can use a linear regression to figure out the effect of the 10 different variables on the progression of the disease as seen below.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn import  datasets, linear_model, tree\n\ndef plotDiabetes1(setNumber, xlabel):\n\n    diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n\n    diabetes_X = diabetes_X[:, np.newaxis, setNumber]\n\n    diabetes_X_train = diabetes_X[:-20]\n    diabetes_X_test = diabetes_X[-20:]\n\n    diabetes_y_train = diabetes_y[:-20]\n    diabetes_y_test = diabetes_y[-20:]\n\n    regr = linear_model.LinearRegression()\n\n    regr.fit(diabetes_X_train, diabetes_y_train)\n\n    diabetes_y_pred = regr.predict(diabetes_X_test)\n\n    #ax= plt.subplot(2,2,3)\n    plt.scatter(diabetes_X_test, diabetes_y_test, color=\"black\")\n    plt.plot(diabetes_X_test, diabetes_y_pred, color=\"blue\", linewidth=3)\n\n    plt.title(\"Impact of \" + xlabel)\n    plt.xlabel(\"Standardized \" + xlabel)\n    plt.ylabel(\"target\")\n    plt.grid()\n    plt.show()\n    \n    \n\n    \nbmiPlot = plotDiabetes1(2, \"BMI\")\nbpPlot = plotDiabetes1(3, \"Blood Pressure\")\nagePlot = plotDiabetes1(0, \"Age\")\n```\n\n::: {.cell-output .cell-output-display}\n![](linearAndNonlinear_files/figure-html/cell-2-output-1.png){width=593 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](linearAndNonlinear_files/figure-html/cell-2-output-2.png){width=593 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](linearAndNonlinear_files/figure-html/cell-2-output-3.png){width=593 height=449}\n:::\n:::\n\n\nThese graphs are scatters of the test data with an overlay of the regression line on top. From this we can see that there is some trend, but there is a lot of variance. The regression being fit helps us to better understand the relationship. We can see that each of these lines has a positive slope telling us that there is some relationship between the progression and the different factors.\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\ndef plotDiabetes2(setNumber, xlabel, color):\n\n    diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n\n    diabetes_X = diabetes_X[:, np.newaxis, setNumber]\n\n    diabetes_X_train = diabetes_X[:-20]\n    diabetes_X_test = diabetes_X[-20:]\n\n    diabetes_y_train = diabetes_y[:-20]\n    diabetes_y_test = diabetes_y[-20:]\n\n    regr = linear_model.LinearRegression()\n\n    regr.fit(diabetes_X_train, diabetes_y_train)\n\n    diabetes_y_pred = regr.predict(diabetes_X_test)\n\n    ax= plt.subplot(2,2,3)\n    #plt.scatter(diabetes_X_test, diabetes_y_test, color=\"black\")\n    plt.plot(diabetes_X_test, diabetes_y_pred, color= color, linewidth=3, label= xlabel)\n    \n    \n    \n    plt.legend(loc=\"upper left\")\n    plt.title(\"Impact of Different Contributers\")\n    plt.xlabel(\"Standardized Markers\")\n    plt.ylabel(\"target\")\n    plt.grid()\n    \n    \n\n    \nbmiPlot = plotDiabetes2(2, \"BMI\", \"Blue\")\nbpPlot = plotDiabetes2(3, \"BP\", \"Red\")\nagePlot = plotDiabetes2(0, \"Age\", \"Black\")\n```\n\n::: {.cell-output .cell-output-display}\n![](linearAndNonlinear_files/figure-html/cell-3-output-1.png){width=319 height=248}\n:::\n:::\n\n\nIf we plot the regression lines of the three factors all on one graph we can get an idea of which factors have more impact on the outcomes of the patients. From this though we can tell that each one has some impact we can very clearly see that BMI and blood pressure are better indicators of the progression of the disease over the course of a year. This information can give researchers a better idea of what factors to look into and doctors ideas of what factors they should look for in patients that come to see them.\n\nA linear regression might work very well for certain applications, but sometimes it is not enough to truly understand the complexity of the problem. So we can also use non-linear approaches to help us with these other problems. A good example of one of these techniques would be a decision tree. While a linear regression is great for a few variables with one specific target outcome, a decision tree can help us classify data into different groups to help demonstrate trends in the data that may be more complex to recognize.\n\nTo demonstrate this, I will be using the penguins data set provided in Seaborn. The data represents three species of penguins and the general characteristics of 344 penguins of those species. We can look at these characteristics and use them to try and determine if there are any specific parameters that distinguish the different species. A decision tree can help us with this as it will go through the three given parameters (bill length/depth and flipper length) and try to determine any distinct differences to break up the data between species. Due to the fact that a decision tree breaks up its processing into different if/then statements it does not work in a linear fashion like the linear regression does.\n\n::: {.cell tags='[]' execution_count=3}\n``` {.python .cell-code}\nimport seaborn as sb\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.tree import plot_tree\n\n\npenguins = sb.load_dataset(\"penguins\")\n\ny= penguins.species\nx= np.array([[penguins.bill_length_mm, penguins.bill_depth_mm, penguins.flipper_length_mm]])\nx = x.reshape(x.shape[1:])\nx = x.transpose()\n\nlen(penguins.species.values)\n\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=27) \n\nclf = DecisionTreeClassifier(max_depth=2, random_state=42)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n\n\n\n\n\n\nplot_tree(\n    clf,\n    filled=True,\n    feature_names=[\"Bill Length (mm)\", \"Bill Depth (mm)\", \"Flipper Length (mm)\"],\n    rounded=True,\n    \n);\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.9903846153846154\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](linearAndNonlinear_files/figure-html/cell-4-output-2.png){width=540 height=389}\n:::\n:::\n\n\nAbove you can see the tree and how it made its decision to break up the penguins into three distinct class. You can see that initially it compared the flipper length, then it went to bill depth to try and break apart the data. If we needed we could have the tree go through more branches to try to see if the results could get better. However, we want to be careful to not over fit the data. To tell how well the model is doing we can take a look at the accuracy by comparing the actual results of the test data to the predicted values. From this as seen above, we can see that the accuracy is 99% which is fairly good. With this in mind we would not want to go deeper with the tree, but we can see that the tree has done a great job of helping us analyze this data in a non-linear manner.\n\nReferences:\n\nInformation for diabetes data set taken from:\n\nhttps://rowannicholls.github.io/python/data/sklearn_datasets/diabetes.html\n\nhttps://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset\n\nInformation for penguins database and decision trees taken from:\n\nhttps://github.com/ageron/handson-ml3\n\nhttps://www.datacamp.com/tutorial/decision-tree-classification-python\n\nhttps://seaborn.pydata.org/archive/0.11/tutorial/function_overview.html\n\n",
    "supporting": [
      "linearAndNonlinear_files"
    ],
    "filters": [],
    "includes": {}
  }
}