---
title: Classification with Decision Trees
jupyter: python3
format: 
    html:
      code-fold: true
---

Classification is a very important technique for helping us understand data and use that information in the future. The goal of classification is to take a set of data and break it up into distinct groups based on the characteristics in the data set. Typically, the data is broken up into a testing set and a training set. The training set is used to train the classification model on how to predict what characteristics distinguish between the different groups.

For example, one of the most common toy data sets used is the flowers data set. This contains the physical characteristics of three different types of flowers which can then be used to determine what a type a data point may be. An analyst may take the data set and take 70% of the data evenly from the set to train the model. The remaining data can be used later to verify the accuracy of the model once the parameters have been fit. We can use the predicted flower type from the model to compare to the true value and check the accuracy of it.

There are many different models we could use to do this process for example: logistic regression, decision trees, support vector machines, random forests, and many more. To help demonstrate this though, I will be using a decision tree classification and the wine data set that is pre-loaded into python with Sklearn.

```{python}
#| tags: []
import seaborn as sb
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import numpy as np
from sklearn import metrics
from sklearn.datasets import load_wine
from sklearn.tree import DecisionTreeClassifier

wine = load_wine(as_frame=True)
X_wine = wine.data[["alcohol", "ash"]].values
y_wine = wine.target

X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, test_size=0.3, random_state=27) 
tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf.fit(X_train, y_train)
y_pred = tree_clf.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

plot_tree(
    tree_clf,
    filled=True,
    feature_names=["Alcohol", "Ash"],
    rounded=True,
    
);
```

As you can see above, the decision tree takes the set and starts from the top breaking up the data based on different decisions the model makes. Once it has broken up the wines based on alcohol it moves on to break it up further based on the ash characteristic. If we let it the tree could go through another level of decision making to try and try to split up the data better. However, it is a good idea to take a look at the accuracy of the model to see if ash and alcohol are good determining characteristics. The accuracy score is 62% which is better than just randomly guessing, but not great.

```{python}
#| tags: []
import numpy as np
import matplotlib.pyplot as plt


from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])
plt.figure(figsize=(8, 4))

lengths, widths = np.meshgrid(np.linspace(0, 7.2, 100), np.linspace(0, 3, 100))
X_wine_all = np.c_[lengths.ravel(), widths.ravel()]
y_pred = tree_clf.predict(X_wine_all).reshape(lengths.shape)

for idx, (name, style) in enumerate(zip(wine.target_names, ("yo", "bs", "g^"))):
    plt.plot(X_wine[:, 0][y_wine == idx], X_wine[:, 1][y_wine == idx],
             style, label=f"wine {name}")


tree_clf_deeper = DecisionTreeClassifier(max_depth=3, random_state=42)
tree_clf_deeper.fit(X_wine, y_wine)

plt.xlabel("Alcohol")
plt.ylabel("Ash")

plt.legend()


plt.show()
```

If we make a scatter plot of the data using the alcohol and ash information we can see that the alcohol does seem to split up the data some. However, the ash seems to really have no impact on the data at all. Based on this we need to find a different secondary characteristic to help us break up the data.

We can try making another tree using the hue characteristic instead of ash.

```{python}
#| tags: []
import numpy as np
import matplotlib.pyplot as plt

wine = load_wine(as_frame=True)
X_wine2 = wine.data[["alcohol", "hue"]].values
y_wine2 = wine.target

X_train2, X_test2, y_train2, y_test2 = train_test_split(X_wine2, y_wine2, test_size=0.3, random_state=27) 
tree_clf2 = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf2.fit(X_train2, y_train2)
y_pred2 = tree_clf2.predict(X_test2)

from matplotlib.colors import ListedColormap
custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])
plt.figure(figsize=(8, 4))

lengths, widths = np.meshgrid(np.linspace(0, 7.2, 100), np.linspace(0, 3, 100))
X_wine_all = np.c_[lengths.ravel(), widths.ravel()]
y_pred2 = tree_clf2.predict(X_wine_all).reshape(lengths.shape)

for idx, (name, style) in enumerate(zip(wine.target_names, ("yo", "bs", "g^"))):
    plt.plot(X_wine2[:, 0][y_wine2 == idx], X_wine2[:, 1][y_wine2 == idx],
             style, label=f"wine {name}")


tree_clf_deeper = DecisionTreeClassifier(max_depth=3, random_state=42)
tree_clf_deeper.fit(X_wine2, y_wine2)

plt.xlabel("Alcohol")
plt.ylabel("Hue")

plt.legend()


plt.show()
```

The above plot is another scatter plot, but this time with the hue plotted against it. From this we can see that the hue clearly plays a roll in separating the data, so the decision tree should be able to better break up the set.

```{python}
#| tags: []
wine = load_wine(as_frame=True)
X_wine2 = wine.data[["alcohol", "hue"]].values
y_wine2 = wine.target

X_train2, X_test2, y_train2, y_test2 = train_test_split(X_wine2, y_wine2, test_size=0.3, random_state=27) 
tree_clf2 = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf2.fit(X_train2, y_train2)
y_pred2 = tree_clf2.predict(X_test2)

print("Accuracy:",metrics.accuracy_score(y_test2, y_pred2))

plot_tree(
    tree_clf2,
    filled=True,
    feature_names=["Alcohol", "Hue"],
    rounded=True,
    
);
```

The above tree was generated using the hue and alcohol content of the wines. You can see that at the first step the wines were broken up by alcohol which mostly broke up classes 0 and 1. Then at the second step the data was broken up by hue which pulled out most of class 2. This can be seen with the accuracy which is now at 90%. This is significantly higher than the previous attempt which proves that the hue was the correct call for this data set.

This decision tree tells us that the data can be effectively broken up by hue and alcohol, but there are other characteristics we could also try to help us break up the data if we wanted to work on getting the accuracy higher. We could also try different models to see if one might fit the data a bit better due to how they work through the data. These techniques can be used to help researchers better understand the realities around us every day much more quickly then by traditional manual work.
